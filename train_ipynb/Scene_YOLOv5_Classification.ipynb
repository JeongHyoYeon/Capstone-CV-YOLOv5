{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1KZiKUAjtARHAfZCXbJRv14-pOnIsBLPV","timestamp":1681452850628}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# YOLOv5 Classification Tutorial\n","\n","YOLOv5 supports classification tasks too. This is the official YOLOv5 classification notebook tutorial. YOLOv5 is maintained by [Ultralytics](https://github.com/ultralytics/yolov5).\n","\n","This notebook covers:\n","\n","*   Inference with out-of-the-box YOLOv5 classification on ImageNet\n","*  [Training YOLOv5 classification](https://blog.roboflow.com//train-YOLOv5-classification-custom-data) on custom data\n","\n","*Looking for custom data? Explore over 66M community datasets on [Roboflow Universe](https://universe.roboflow.com).*\n","\n","This notebook was created with Google Colab. [Click here](https://colab.research.google.com/drive/1FiSNz9f_nT8aFtDEU3iDAQKlPT8SCVni?usp=sharing) to run it."],"metadata":{"id":"5GYQX3of4QiW"}},{"cell_type":"markdown","source":["# 개인 google drive에 dataset 다운 후 전처리"],"metadata":{"id":"UW6kcMTPoHHn"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PHxsI2qVnxIM","executionInfo":{"status":"ok","timestamp":1682753853988,"user_tz":-540,"elapsed":22857,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"0344c5dc-497f-4be9-9f87-c59d32b1fa67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install roboflow\n","!pip install split-folders"],"metadata":{"id":"5ea1N8M8ukf8","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1682753871950,"user_tz":-540,"elapsed":14440,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"d129c673-2b93-4923-a984-c92a7e781aa2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting roboflow\n","  Downloading roboflow-1.0.5-py3-none-any.whl (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.2/56.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.0.0)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.65.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.4.4)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.22.4)\n","Collecting cycler==0.10.0\n","  Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.16.0)\n","Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (4.7.0.72)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from roboflow) (3.7.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.27.1)\n","Collecting idna==2.10\n","  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting python-dotenv\n","  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from roboflow) (2.8.2)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from roboflow) (6.0)\n","Collecting pyparsing==2.4.7\n","  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting requests-toolbelt\n","  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from roboflow) (8.4.0)\n","Requirement already satisfied: certifi==2022.12.7 in /usr/local/lib/python3.10/dist-packages (from roboflow) (2022.12.7)\n","Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.10/dist-packages (from roboflow) (1.26.15)\n","Collecting wget\n","  Downloading wget-3.2.zip (10 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (1.0.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (23.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->roboflow) (4.39.3)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->roboflow) (2.0.12)\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9676 sha256=1feda70f38794192462897bac6bec4a35715c56b1009f881a01347daa60f705e\n","  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n","Successfully built wget\n","Installing collected packages: wget, python-dotenv, pyparsing, idna, cycler, requests-toolbelt, roboflow\n","  Attempting uninstall: pyparsing\n","    Found existing installation: pyparsing 3.0.9\n","    Uninstalling pyparsing-3.0.9:\n","      Successfully uninstalled pyparsing-3.0.9\n","  Attempting uninstall: idna\n","    Found existing installation: idna 3.4\n","    Uninstalling idna-3.4:\n","      Successfully uninstalled idna-3.4\n","  Attempting uninstall: cycler\n","    Found existing installation: cycler 0.11.0\n","    Uninstalling cycler-0.11.0:\n","      Successfully uninstalled cycler-0.11.0\n","Successfully installed cycler-0.10.0 idna-2.10 pyparsing-2.4.7 python-dotenv-1.0.0 requests-toolbelt-0.10.1 roboflow-1.0.5 wget-3.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["cycler","pyparsing"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting split-folders\n","  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n","Installing collected packages: split-folders\n","Successfully installed split-folders-0.5.1\n"]}]},{"cell_type":"code","source":["# 개인 드라이브에 dataset download \n","# train 폴더 뿐.\n","import os\n","original_data_path = \"/content/drive/MyDrive/Capstone_Yolov5/original_dataset/\"\n","os.makedirs(original_data_path, exist_ok=True)\n","%cd $original_data_path\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"48MwSygwi4od6wzAwKIZ\")\n","project = rf.workspace(\"m3-ytsk5\").project(\"m3finalclass\")\n","dataset = project.version(1).download(\"folder\")"],"metadata":{"id":"pwaFPQy3qOA0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682251563661,"user_tz":-540,"elapsed":73584,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"ab8feb14-44cf-4bf2-a11d-ce0d8ae14b08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Capstone_Yolov5/original_dataset\n","loading Roboflow workspace...\n","loading Roboflow project...\n","Downloading Dataset Version Zip in m3finalclass-1 to folder: 100% [13983137 / 13983137] bytes\n"]},{"output_type":"stream","name":"stderr","text":["Extracting Dataset Version Zip to m3finalclass-1 in folder:: 100%|██████████| 1211/1211 [00:07<00:00, 161.51it/s]\n"]}]},{"cell_type":"code","source":["# 개인 드라이브에 dataset split\n","# train, test, valid 로 split\n","data_path = \"/content/drive/MyDrive/Capstone_Yolov5/dataset_scene/\"\n","os.makedirs(data_path, exist_ok=True)\n","%cd $data_path\n","\n","import splitfolders\n","splitfolders.ratio(original_data_path + \"m3finalclass-1/train\", output=data_path, seed=77, ratio=(0.8, 0.1, 0.1))"],"metadata":{"id":"80Mo_Nxruja0","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"error","timestamp":1682753879662,"user_tz":-540,"elapsed":406,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"de6f9622-51b7-416c-e5e4-8c4617c8ee65"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-7188b81dd00d>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# train, test, valid 로 split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Capstone_Yolov5/dataset_object/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'$data_path'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"]}]},{"cell_type":"markdown","source":["# Setup\n","\n","Pull in respective libraries to prepare the notebook environment."],"metadata":{"id":"-PJ8vlYXbWtN"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"pIM7fOwm8A7l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682250568094,"user_tz":-540,"elapsed":13500,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"4c420911-52ad-4bf7-d983-eeae530f3474"},"outputs":[{"output_type":"stream","name":"stderr","text":["YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n"]},{"output_type":"stream","name":"stdout","text":["Setup complete ✅ (2 CPUs, 12.7 GB RAM, 23.3/78.2 GB disk)\n"]}],"source":["!git clone https://github.com/ultralytics/yolov5  # clone\n","%cd yolov5\n","%pip install -qr requirements.txt  # install\n","\n","import torch\n","import utils\n","display = utils.notebook_init()  # checks"]},{"cell_type":"markdown","source":["# 1. Infer on ImageNet\n","\n","To demonstrate YOLOv5 classification, we'll leverage an already trained model. In this case, we'll download the ImageNet trained models pretrained on ImageNet using YOLOv5 Utils."],"metadata":{"id":"i_DrUi2nmF40"}},{"cell_type":"code","source":["from utils.downloads import attempt_download\n","\n","p5 = ['n', 's', 'm', 'l', 'x']  # P5 models\n","cls = [f'{x}-cls' for x in p5]  # classification models\n","\n","for x in cls:\n","    attempt_download(f'weights/yolov5{x}.pt')"],"metadata":{"id":"o2scLEh6EYnL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682250575539,"user_tz":-540,"elapsed":4362,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"fa618cf9-1dcd-4f47-8199-dc98df3a9e00"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n-cls.pt to weights/yolov5n-cls.pt...\n","100%|██████████| 4.87M/4.87M [00:00<00:00, 63.4MB/s]\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to weights/yolov5s-cls.pt...\n","100%|██████████| 10.5M/10.5M [00:00<00:00, 84.3MB/s]\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5m-cls.pt to weights/yolov5m-cls.pt...\n","100%|██████████| 24.9M/24.9M [00:00<00:00, 133MB/s]\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5l-cls.pt to weights/yolov5l-cls.pt...\n","100%|██████████| 50.9M/50.9M [00:00<00:00, 151MB/s]\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x-cls.pt to weights/yolov5x-cls.pt...\n","100%|██████████| 92.0M/92.0M [00:01<00:00, 86.8MB/s]\n","\n"]}]},{"cell_type":"markdown","source":["Now, we can infer on an example image from the ImageNet dataset."],"metadata":{"id":"Fn2_a38DmZ2H"}},{"cell_type":"code","source":["#Download example image\n","import requests\n","image_url = \"https://i.imgur.com/OczPfaz.jpg\"\n","img_data = requests.get(image_url).content\n","with open('bananas.jpg', 'wb') as handler:\n","    handler.write(img_data)"],"metadata":{"id":"L9objhVHnS-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Infer using classify/predict.py\n","!python classify/predict.py --weights ./weigths/yolov5s-cls.pt --source bananas.jpg"],"metadata":{"id":"qqxF5pHCrLd3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682250603000,"user_tz":-540,"elapsed":8563,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"99890e6f-5e88-4d38-85bd-983de7f9b271"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['./weigths/yolov5s-cls.pt'], source=bananas.jpg, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=False, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\n","\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n","YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s-cls.pt to weigths/yolov5s-cls.pt...\n","100% 10.5M/10.5M [00:00<00:00, 110MB/s]\n","\n","Fusing layers... \n","Model summary: 117 layers, 5447688 parameters, 0 gradients, 11.4 GFLOPs\n","image 1/1 /content/yolov5/bananas.jpg: 224x224 banana 0.96, zucchini 0.00, acorn squash 0.00, spaghetti squash 0.00, green mamba 0.00, 5.5ms\n","Speed: 0.6ms pre-process, 5.5ms inference, 29.7ms NMS per image at shape (1, 3, 224, 224)\n","Results saved to \u001b[1mruns/predict-cls/exp\u001b[0m\n"]}]},{"cell_type":"markdown","source":["From the output, we can see the ImageNet trained model correctly predicts the class `banana` with `0.95` confidence."],"metadata":{"id":"yQmj7IXqo3kk"}},{"cell_type":"markdown","source":["## 2. (Optional) Validate\n","\n","Use the `classify/val.py` script to run validation for the model. This will show us the model's performance on each class.\n","\n","First, we need to download ImageNet."],"metadata":{"id":"5EosQzyDCk3W"}},{"cell_type":"code","source":["# # WARNING: takes ~20 minutes\n","# !bash data/scripts/get_imagenet.sh --val"],"metadata":{"id":"HwAYptjCq-C_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # run the validation script\n","# !python classify/val.py --weights ./weigths/yolov5s-cls.pt --data ../datasets/imagenet"],"metadata":{"id":"CoHdKXWc8hrD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The output shows accuracy metrics for the ImageNet validation dataset including per class accuracy."],"metadata":{"id":"r2coOcIjuzCO"}},{"cell_type":"markdown","source":["# 3. Train On Custom Data\n","\n","To train on custom data, we need to prepare a dataset with custom labels.\n","\n","To prepare custom data, we'll use [Roboflow](https://roboflow.com). Roboflow enables easy dataset prep with your team, including labeling, formatting into the right export format, deploying, and active learning with a `pip` package. \n","\n","If you need custom data, there are over 66M open source images from the community on [Roboflow Universe](https://universe.roboflow.com).\n","\n","(For more guidance, here's a detailed blog on [training YOLOv5 classification on custom data](https://blog.roboflow.com/train-YOLOv5-classification-custom-data).)\n","\n","\n","Create a free Roboflow account, upload your data, and label. \n","\n","![](https://s4.gifyu.com/images/fruit-labeling.gif)"],"metadata":{"id":"9bXHHYeVDCXg"}},{"cell_type":"markdown","source":["### Load Custom Dataset\n","\n","Next, we'll export our dataset into the right directory structure for training YOLOv5 classification to load into this notebook. Select the `Export` button at the top of the version page, `Folder Structure` type, and `show download code`.\n","\n","The ensures all our directories are in the right format:\n","\n","```\n","dataset\n","├── train\n","│   ├── class-one\n","│   │   ├── IMG_123.jpg\n","│   └── class-two\n","│       ├── IMG_456.jpg\n","├── valid\n","│   ├── class-one\n","│   │   ├── IMG_789.jpg\n","│   └── class-two\n","│       ├── IMG_101.jpg\n","├── test\n","│   ├── class-one\n","│   │   ├── IMG_121.jpg\n","│   └── class-two\n","│       ├── IMG_341.jpg\n","```\n","\n","![](https://i.imgur.com/BF9BNR8.gif)\n","\n","\n","Copy and paste that snippet into the cell below."],"metadata":{"id":"Cu6-lrukD6Hc"}},{"cell_type":"code","source":["# Ensure we're in the right directory to download our custom dataset\n","'''\n","import os\n","os.makedirs(\"../datasets/\", exist_ok=True)\n","%cd ../datasets/\n","'''"],"metadata":{"id":"6IIgJbP7G6Th","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1682250686021,"user_tz":-540,"elapsed":5,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"51322e74-bae7-4708-f18d-bb71eaf3886c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nimport os\\nos.makedirs(\"../datasets/\", exist_ok=True)\\n%cd ../datasets/\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# REPLACE the below with your exported code snippet from above\n","#이제 dataset 다운 받지 말고 개인드라이브에서 전처리완료된 데이터셋 데려오자.\n","'''\n","!pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"YOUR API KEY\")\n","project = rf.workspace(\"yolov5-classification\").project(\"banana-ripeness-classification\")\n","dataset = project.version(1).download(\"folder\")\n","'''\n","\n","'''\n","!pip install roboflow\n","\n","from roboflow import Roboflow\n","rf = Roboflow(api_key=\"48MwSygwi4od6wzAwKIZ\")\n","project = rf.workspace(\"m3-ytsk5\").project(\"m3finalclass\")\n","dataset = project.version(1).download(\"folder\")\n","'''"],"metadata":{"id":"He6JwHIlG-W_","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1682250684446,"user_tz":-540,"elapsed":320,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"731fc79c-08d0-4272-d915-e760447544fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n!pip install roboflow\\n\\nfrom roboflow import Roboflow\\nrf = Roboflow(api_key=\"48MwSygwi4od6wzAwKIZ\")\\nproject = rf.workspace(\"m3-ytsk5\").project(\"m3finalclass\")\\ndataset = project.version(1).download(\"folder\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#Save the dataset name to the environment so we can use it in a system call later\n","'''\n","dataset_name = dataset.location.split(os.sep)[-1]\n","os.environ[\"DATASET_NAME\"] = dataset_name\n","'''"],"metadata":{"id":"wLQbThFICpn4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","DATASET_PATH = \"/content/drive/MyDrive/Capstone_Yolov5/dataset_scene\"\n","\n","DATASET_NAME = DATASET_PATH\n","os.environ[\"DATASET_NAME\"] = DATASET_NAME"],"metadata":{"id":"y3InBLkA5irC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train On Custom Data 🎉\n","Here, we use the DATASET_NAME environment variable to pass our dataset to the `--data` parameter.\n","\n","Note: we're training for 100 epochs here. We're also starting training from the pretrained weights. Larger datasets will likely benefit from longer training. "],"metadata":{"id":"-5z7Yv42FGrK"}},{"cell_type":"code","source":["%cd \"/content/yolov5\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NtmHWf8TlN52","executionInfo":{"status":"ok","timestamp":1682252071403,"user_tz":-540,"elapsed":2,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"e539fc15-634a-44c7-82db-2ee16d2477d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov5\n"]}]},{"cell_type":"code","source":["#  Epoch 원래 100\n","%cd ../yolov5\n","!python classify/train.py --model yolov5s-cls.pt --data $DATASET_NAME --epochs 100 --img 128 --pretrained weights/yolov5s-cls.pt"],"metadata":{"id":"MXWTTN2BEaqe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682252475809,"user_tz":-540,"elapsed":354058,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"048a4dd3-5862-4cf5-d8d6-a503ef073fc3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/yolov5\n","\u001b[34m\u001b[1mclassify/train: \u001b[0mmodel=yolov5s-cls.pt, data=/content/drive/MyDrive/Capstone_Yolov5/dataset_scene, epochs=100, batch_size=64, imgsz=128, nosave=False, cache=None, device=, workers=8, project=runs/train-cls, name=exp, exist_ok=False, pretrained=weights/yolov5s-cls.pt, optimizer=Adam, lr0=0.001, decay=5e-05, label_smoothing=0.1, cutoff=None, dropout=None, verbose=False, seed=0, local_rank=-1\n","\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n","\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n","YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train-cls', view at http://localhost:6006/\n","\u001b[34m\u001b[1malbumentations: \u001b[0mRandomResizedCrop(p=1.0, height=128, width=128, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333333333), interpolation=1), HorizontalFlip(p=0.5), ColorJitter(p=0.5, brightness=[0.6, 1.4], contrast=[0.6, 1.4], saturation=[0.6, 1.4], hue=[0, 0]), Normalize(p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0), ToTensorV2(always_apply=True, p=1.0, transpose_mask=False)\n","Model summary: 149 layers, 4182728 parameters, 4182728 gradients, 10.5 GFLOPs\n","\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.001) with parameter groups 32 weight(decay=0.0), 33 weight(decay=5e-05), 33 bias\n","Image sizes 128 train, 128 test\n","Using 1 dataloader workers\n","Logging results to \u001b[1mruns/train-cls/exp4\u001b[0m\n","Starting yolov5s-cls.pt training on /content/drive/MyDrive/Capstone_Yolov5/dataset_scene dataset with 8 classes for 100 epochs...\n","\n","     Epoch   GPU_mem  train_loss   test_loss    top1_acc    top5_acc\n","     1/100    0.732G        2.12        2.08       0.125       0.625: 100% 15/15 [00:03<00:00,  4.11it/s]\n","     2/100    0.732G        1.86         2.1       0.125       0.633: 100% 15/15 [00:02<00:00,  5.66it/s]\n","     3/100    0.732G        1.67         2.1       0.192       0.642: 100% 15/15 [00:02<00:00,  5.35it/s]\n","     4/100    0.732G        1.61        2.51       0.142       0.633: 100% 15/15 [00:03<00:00,  4.30it/s]\n","     5/100    0.732G        1.54        2.22       0.233       0.817: 100% 15/15 [00:03<00:00,  4.46it/s]\n","     6/100    0.732G        1.54        1.84       0.358       0.933: 100% 15/15 [00:02<00:00,  5.41it/s]\n","     7/100    0.732G        1.46         1.4       0.558       0.967: 100% 15/15 [00:02<00:00,  5.54it/s]\n","     8/100    0.732G        1.45        1.38       0.567       0.983: 100% 15/15 [00:03<00:00,  4.61it/s]\n","     9/100    0.732G        1.46         1.5       0.508       0.967: 100% 15/15 [00:03<00:00,  3.94it/s]\n","    10/100    0.732G        1.41        1.48       0.525       0.967: 100% 15/15 [00:02<00:00,  5.13it/s]\n","    11/100    0.732G        1.42        1.39       0.608        0.95: 100% 15/15 [00:02<00:00,  5.30it/s]\n","    12/100    0.732G        1.37        1.38       0.575       0.983: 100% 15/15 [00:02<00:00,  5.03it/s]\n","    13/100    0.732G        1.27        1.33       0.575       0.992: 100% 15/15 [00:03<00:00,  3.78it/s]\n","    14/100    0.732G        1.29        1.27       0.675       0.983: 100% 15/15 [00:02<00:00,  5.01it/s]\n","    15/100    0.732G        1.27        1.22       0.675       0.992: 100% 15/15 [00:02<00:00,  5.20it/s]\n","    16/100    0.732G        1.22        1.34       0.642       0.983: 100% 15/15 [00:02<00:00,  5.24it/s]\n","    17/100    0.732G        1.18        1.21       0.633       0.992: 100% 15/15 [00:03<00:00,  3.77it/s]\n","    18/100    0.732G        1.17        1.33       0.633           1: 100% 15/15 [00:03<00:00,  4.92it/s]\n","    19/100    0.732G        1.16         1.2       0.683       0.992: 100% 15/15 [00:02<00:00,  5.17it/s]\n","    20/100    0.732G        1.14        1.36       0.617       0.975: 100% 15/15 [00:02<00:00,  5.34it/s]\n","    21/100    0.732G        1.13        1.12       0.717       0.992: 100% 15/15 [00:03<00:00,  3.98it/s]\n","    22/100    0.732G        1.11        1.22       0.692       0.975: 100% 15/15 [00:03<00:00,  4.90it/s]\n","    23/100    0.732G        1.09       0.971       0.833           1: 100% 15/15 [00:02<00:00,  5.03it/s]\n","    24/100    0.732G        1.08        1.05       0.742           1: 100% 15/15 [00:02<00:00,  5.42it/s]\n","    25/100    0.732G        1.08        1.14       0.708       0.975: 100% 15/15 [00:03<00:00,  4.19it/s]\n","    26/100    0.732G        1.09        1.34       0.658       0.975: 100% 15/15 [00:03<00:00,  4.52it/s]\n","    27/100    0.732G        1.08        1.24       0.633       0.983: 100% 15/15 [00:02<00:00,  5.33it/s]\n","    28/100    0.732G        1.02        1.09        0.75           1: 100% 15/15 [00:02<00:00,  5.02it/s]\n","    29/100    0.732G        1.07        1.22        0.65           1: 100% 15/15 [00:03<00:00,  4.67it/s]\n","    30/100    0.732G        1.02       0.907         0.8       0.992: 100% 15/15 [00:03<00:00,  4.21it/s]\n","    31/100    0.732G        1.02       0.956       0.792           1: 100% 15/15 [00:02<00:00,  5.14it/s]\n","    32/100    0.732G       0.995       0.936       0.817           1: 100% 15/15 [00:02<00:00,  5.05it/s]\n","    33/100    0.732G        1.01       0.907       0.833       0.992: 100% 15/15 [00:03<00:00,  4.77it/s]\n","    34/100    0.732G        1.06       0.997       0.758           1: 100% 15/15 [00:03<00:00,  4.12it/s]\n","    35/100    0.732G        1.04        1.02       0.767           1: 100% 15/15 [00:02<00:00,  5.30it/s]\n","    36/100    0.732G           1       0.978       0.758       0.983: 100% 15/15 [00:02<00:00,  5.01it/s]\n","    37/100    0.732G        0.99        0.98       0.767       0.992: 100% 15/15 [00:02<00:00,  5.24it/s]\n","    38/100    0.732G       0.942       0.948       0.783           1: 100% 15/15 [00:03<00:00,  3.91it/s]\n","    39/100    0.732G       0.959        1.05       0.725           1: 100% 15/15 [00:03<00:00,  4.88it/s]\n","    40/100    0.732G       0.999        1.03        0.75           1: 100% 15/15 [00:02<00:00,  5.23it/s]\n","    41/100    0.732G       0.945        1.04       0.733           1: 100% 15/15 [00:03<00:00,  4.82it/s]\n","    42/100    0.732G       0.943       0.979       0.783           1: 100% 15/15 [00:03<00:00,  4.07it/s]\n","    43/100    0.732G       0.915       0.896       0.825           1: 100% 15/15 [00:03<00:00,  4.37it/s]\n","    44/100    0.732G       0.933       0.893       0.825           1: 100% 15/15 [00:02<00:00,  5.13it/s]\n","    45/100    0.732G       0.915       0.924       0.808       0.992: 100% 15/15 [00:02<00:00,  5.17it/s]\n","    46/100    0.732G        0.88       0.807       0.875           1: 100% 15/15 [00:03<00:00,  4.91it/s]\n","    47/100    0.732G       0.918        1.11       0.742           1: 100% 15/15 [00:03<00:00,  4.14it/s]\n","    48/100    0.732G       0.896       0.881       0.825           1: 100% 15/15 [00:02<00:00,  5.25it/s]\n","    49/100    0.732G       0.918       0.853       0.808           1: 100% 15/15 [00:02<00:00,  5.07it/s]\n","    50/100    0.732G       0.894       0.844       0.842       0.992: 100% 15/15 [00:03<00:00,  4.95it/s]\n","    51/100    0.732G       0.872       0.888       0.833           1: 100% 15/15 [00:03<00:00,  4.00it/s]\n","    52/100    0.732G       0.861       0.886       0.817           1: 100% 15/15 [00:02<00:00,  5.29it/s]\n","    53/100    0.732G       0.832       0.821       0.858           1: 100% 15/15 [00:02<00:00,  5.06it/s]\n","    54/100    0.732G       0.872        0.87       0.825           1: 100% 15/15 [00:03<00:00,  4.97it/s]\n","    55/100    0.732G       0.837       0.874       0.842           1: 100% 15/15 [00:03<00:00,  3.83it/s]\n","    56/100    0.732G       0.832       0.757         0.9           1: 100% 15/15 [00:03<00:00,  4.88it/s]\n","    57/100    0.732G       0.865       0.799        0.85           1: 100% 15/15 [00:02<00:00,  5.08it/s]\n","    58/100    0.732G       0.884       0.768       0.883           1: 100% 15/15 [00:02<00:00,  5.32it/s]\n","    59/100    0.732G         0.8       0.767       0.883           1: 100% 15/15 [00:03<00:00,  4.18it/s]\n","    60/100    0.732G       0.827       0.743       0.892           1: 100% 15/15 [00:03<00:00,  4.46it/s]\n","    61/100    0.732G       0.821       0.803       0.867           1: 100% 15/15 [00:02<00:00,  5.22it/s]\n","    62/100    0.732G       0.845       0.745       0.883           1: 100% 15/15 [00:02<00:00,  5.10it/s]\n","    63/100    0.732G       0.786       0.776       0.858           1: 100% 15/15 [00:03<00:00,  4.54it/s]\n","    64/100    0.732G       0.791       0.799       0.833           1: 100% 15/15 [00:03<00:00,  4.10it/s]\n","    65/100    0.732G       0.852       0.737       0.908           1: 100% 15/15 [00:02<00:00,  5.30it/s]\n","    66/100    0.732G       0.794       0.805       0.833           1: 100% 15/15 [00:02<00:00,  5.54it/s]\n","    67/100    0.732G       0.829       0.749         0.9           1: 100% 15/15 [00:02<00:00,  5.01it/s]\n","    68/100    0.732G       0.761       0.774       0.883           1: 100% 15/15 [00:03<00:00,  3.90it/s]\n","    69/100    0.732G       0.784       0.749       0.883           1: 100% 15/15 [00:02<00:00,  5.14it/s]\n","    70/100    0.732G       0.784       0.728       0.925       0.992: 100% 15/15 [00:02<00:00,  5.21it/s]\n","    71/100    0.732G       0.793       0.719         0.9           1: 100% 15/15 [00:02<00:00,  5.49it/s]\n","    72/100    0.732G       0.816       0.793       0.892           1: 100% 15/15 [00:03<00:00,  4.01it/s]\n","    73/100    0.732G        0.76       0.703       0.858           1: 100% 15/15 [00:03<00:00,  4.63it/s]\n","    74/100    0.732G       0.748       0.703       0.883           1: 100% 15/15 [00:02<00:00,  5.24it/s]\n","    75/100    0.732G       0.755       0.739       0.892           1: 100% 15/15 [00:02<00:00,  5.28it/s]\n","    76/100    0.732G       0.778       0.727       0.917           1: 100% 15/15 [00:03<00:00,  4.16it/s]\n","    77/100    0.732G       0.782       0.668       0.925           1: 100% 15/15 [00:03<00:00,  4.40it/s]\n","    78/100    0.732G       0.733       0.693       0.908           1: 100% 15/15 [00:02<00:00,  5.45it/s]\n","    79/100    0.732G        0.75       0.662       0.917           1: 100% 15/15 [00:02<00:00,  5.18it/s]\n","    80/100    0.732G       0.765       0.714       0.917           1: 100% 15/15 [00:03<00:00,  4.72it/s]\n","    81/100    0.732G       0.747       0.673       0.917           1: 100% 15/15 [00:03<00:00,  4.29it/s]\n","    82/100    0.732G       0.743       0.646       0.942           1: 100% 15/15 [00:02<00:00,  5.03it/s]\n","    83/100    0.732G       0.725       0.651       0.942           1: 100% 15/15 [00:02<00:00,  5.17it/s]\n","    84/100    0.732G       0.718       0.684       0.883           1: 100% 15/15 [00:02<00:00,  5.35it/s]\n","    85/100    0.732G       0.731       0.663       0.917           1: 100% 15/15 [00:04<00:00,  3.49it/s]\n","    86/100    0.732G       0.692        0.64       0.942           1: 100% 15/15 [00:02<00:00,  5.09it/s]\n","    87/100    0.732G       0.731        0.65       0.908           1: 100% 15/15 [00:02<00:00,  5.46it/s]\n","    88/100    0.732G       0.707       0.647       0.933           1: 100% 15/15 [00:02<00:00,  5.08it/s]\n","    89/100    0.732G       0.679       0.636        0.95           1: 100% 15/15 [00:03<00:00,  3.97it/s]\n","    90/100    0.732G       0.669       0.605       0.958           1: 100% 15/15 [00:03<00:00,  4.85it/s]\n","    91/100    0.732G       0.675       0.633       0.942           1: 100% 15/15 [00:02<00:00,  5.27it/s]\n","    92/100    0.732G       0.701       0.668       0.892           1: 100% 15/15 [00:02<00:00,  5.11it/s]\n","    93/100    0.732G       0.726       0.631       0.908           1: 100% 15/15 [00:03<00:00,  4.43it/s]\n","    94/100    0.732G       0.693       0.625       0.933           1: 100% 15/15 [00:03<00:00,  4.45it/s]\n","    95/100    0.732G       0.661       0.622       0.925           1: 100% 15/15 [00:02<00:00,  5.26it/s]\n","    96/100    0.732G       0.694        0.62       0.933           1: 100% 15/15 [00:02<00:00,  5.16it/s]\n","    97/100    0.732G        0.67       0.596       0.967           1: 100% 15/15 [00:03<00:00,  4.79it/s]\n","    98/100    0.732G       0.686       0.601        0.95           1: 100% 15/15 [00:03<00:00,  3.91it/s]\n","    99/100    0.732G       0.676       0.606       0.942           1: 100% 15/15 [00:02<00:00,  5.14it/s]\n","   100/100    0.732G       0.688       0.608       0.942           1: 100% 15/15 [00:03<00:00,  4.94it/s]\n","\n","Training complete (0.094 hours)\n","Results saved to \u001b[1mruns/train-cls/exp4\u001b[0m\n","Predict:         python classify/predict.py --weights runs/train-cls/exp4/weights/best.pt --source im.jpg\n","Validate:        python classify/val.py --weights runs/train-cls/exp4/weights/best.pt --data /content/drive/MyDrive/Capstone_Yolov5/dataset_scene\n","Export:          python export.py --weights runs/train-cls/exp4/weights/best.pt --include onnx\n","PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp4/weights/best.pt')\n","Visualize:       https://netron.app\n","\n"]}]},{"cell_type":"code","source":["# 시각화하기 위해 best.onnx 다운로드\n","!python export.py --weights runs/train-cls/exp4/weights/best.pt --include onnx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8T7sZNwSF93S","executionInfo":{"status":"ok","timestamp":1682253356338,"user_tz":-540,"elapsed":11456,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"8ed4cfcd-8587-4862-abea-cbc34897bbc8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['runs/train-cls/exp4/weights/best.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n","YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CPU\n","\n","Fusing layers... \n","Model summary: 117 layers, 4176936 parameters, 0 gradients, 10.4 GFLOPs\n","\n","\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from runs/train-cls/exp4/weights/best.pt with output shape (1, 8) (8.1 MB)\n","\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"onnx>=1.12.0\" not found, attempting AutoUpdate...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting onnx>=1.12.0\n","  Downloading onnx-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n","     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.5/13.5 MB 98.7 MB/s eta 0:00:00\n","Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.9/dist-packages (from onnx>=1.12.0) (3.20.3)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx>=1.12.0) (1.22.4)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx>=1.12.0) (4.5.0)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.13.1\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per ['onnx>=1.12.0']\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.13.1...\n","============= Diagnostic Run torch.onnx.export version 2.0.0+cu118 =============\n","verbose: False, log level: Level.ERROR\n","======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n","\n","\u001b[34m\u001b[1mONNX:\u001b[0m export success ✅ 6.3s, saved as runs/train-cls/exp4/weights/best.onnx (16.0 MB)\n","\n","Export complete (7.1s)\n","Results saved to \u001b[1m/content/yolov5/runs/train-cls/exp4/weights\u001b[0m\n","Detect:          python classify/predict.py --weights runs/train-cls/exp4/weights/best.onnx \n","Validate:        python classify/val.py --weights runs/train-cls/exp4/weights/best.onnx \n","PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp4/weights/best.onnx')  # WARNING ⚠️ ClassificationModel not yet supported for PyTorch Hub AutoShape inference\n","Visualize:       https://netron.app\n"]}]},{"cell_type":"code","source":["model = torch.hub.load('ultralytics/yolov5', 'custom', 'runs/train-cls/exp4/weights/best.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eyg8vrs8FyXW","executionInfo":{"status":"ok","timestamp":1682253386801,"user_tz":-540,"elapsed":11078,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"2051f6c7-071b-4758-8816-3a6863dc05fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n","YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n"]},{"output_type":"stream","name":"stderr","text":["Fusing layers... \n","Model summary: 117 layers, 4176936 parameters, 0 gradients, 10.4 GFLOPs\n","WARNING ⚠️ YOLOv5 ClassificationModel is not yet AutoShape compatible. You must pass torch tensors in BCHW to this model, i.e. shape(1,3,224,224).\n"]}]},{"cell_type":"markdown","source":["### Validate Your Custom Model\n","\n","Repeat step 2 from above to test and validate your custom model."],"metadata":{"id":"HHUFGeLbGd98"}},{"cell_type":"code","source":["#!python classify/val.py --weights runs/train-cls/exp/weights/best.pt --data ../datasets/$DATASET_NAME\n","#!python classify/val.py --weights runs/train-cls/exp/weights/best.pt --data $DATASET_PATH\n","!python classify/val.py --weights runs/train-cls/exp4/weights/best.pt --data $DATASET_PATH"],"metadata":{"id":"DIV7ydyKGZFL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682253519161,"user_tz":-540,"elapsed":6292,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"626b1200-7a0e-447b-db9f-014ba3d90f7e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mclassify/val: \u001b[0mdata=/content/drive/MyDrive/Capstone_Yolov5/dataset_scene, weights=['runs/train-cls/exp4/weights/best.pt'], batch_size=128, imgsz=224, device=, workers=8, verbose=True, project=runs/val-cls, name=exp, exist_ok=False, half=False, dnn=False\n","\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n","YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 117 layers, 4176936 parameters, 0 gradients, 10.4 GFLOPs\n","testing: 100% 1/1 [00:01<00:00,  1.26s/it]\n","                   Class      Images    top1_acc    top5_acc\n","                     all         120       0.958           1\n","             Opencountry          15         0.8           1\n","                   coast          15           1           1\n","                  forest          15           1           1\n","                 highway          15           1           1\n","             inside_city          15           1           1\n","                mountain          15       0.867           1\n","                  street          15           1           1\n","            tallbuilding          15           1           1\n","Speed: 0.1ms pre-process, 3.9ms inference, 0.2ms post-process per image at shape (1, 3, 224, 224)\n","Results saved to \u001b[1mruns/val-cls/exp3\u001b[0m\n"]}]},{"cell_type":"markdown","source":["### Infer With Your Custom Model"],"metadata":{"id":"uH5tJNpEsi6g"}},{"cell_type":"code","source":["#Get the path of an image from the test or validation set\n","'''\n","if os.path.exists(os.path.join(dataset.location, \"test\")):\n","  split_path = os.path.join(dataset.location, \"test\")\n","else:\n","  os.path.join(dataset.location, \"valid\")\n","'''\n","split_path = DATASET_PATH + \"/test\"\n","example_class = os.listdir(split_path)[0]\n","example_image_name = os.listdir(os.path.join(split_path, example_class))[0]\n","example_image_path = os.path.join(split_path, example_class, example_image_name)\n","os.environ[\"TEST_IMAGE_PATH\"] = example_image_path\n","\n","print(f\"Inferring on an example of the class '{example_class}'\")\n","\n","#Infer\n","#!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source $TEST_IMAGE_PATH\n","!python classify/predict.py --weights runs/train-cls/exp4/weights/best.pt --source $TEST_IMAGE_PATH"],"metadata":{"id":"81lK1hU_sk54","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682253593189,"user_tz":-540,"elapsed":4514,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"4ae235ae-979a-4e19-fbb3-f3155438b93f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Inferring on an example of the class 'Opencountry'\n","\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=/content/drive/MyDrive/Capstone_Yolov5/dataset_scene/test/Opencountry/natu885_jpg.rf.73214c3f227d6ad03665d75e5878efe9.jpg, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=False, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\n","\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n","YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 117 layers, 4176936 parameters, 0 gradients, 10.4 GFLOPs\n","image 1/1 /content/drive/MyDrive/Capstone_Yolov5/dataset_scene/test/Opencountry/natu885_jpg.rf.73214c3f227d6ad03665d75e5878efe9.jpg: 224x224 Opencountry 0.56, mountain 0.19, street 0.08, forest 0.08, highway 0.04, 3.4ms\n","Speed: 0.4ms pre-process, 3.4ms inference, 5.6ms NMS per image at shape (1, 3, 224, 224)\n","Results saved to \u001b[1mruns/predict-cls/exp3\u001b[0m\n"]}]},{"cell_type":"code","source":["# ===== predict 다른 사진으로 한번더!\n","\n","# test 폴더에서 사진 한장 가지고 오기\n","split_path = DATASET_PATH + \"/test\"\n","example_class = os.listdir(split_path)[1]\n","example_image_name = os.listdir(os.path.join(split_path, example_class))[0]\n","example_image_path = os.path.join(split_path, example_class, example_image_name)\n","os.environ[\"TEST_IMAGE_PATH\"] = example_image_path\n","\n","print(f\"Inferring on an example of the class '{example_class}'\")\n","\n","# 그 사진으로 Infer\n","!python classify/predict.py --weights runs/train-cls/exp4/weights/best.pt --source $TEST_IMAGE_PATH"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cr_7_CBLKhfF","executionInfo":{"status":"ok","timestamp":1682254283196,"user_tz":-540,"elapsed":4741,"user":{"displayName":"임연우","userId":"08836425153877533305"}},"outputId":"9726154d-9e75-45cc-e4e5-1d01c97d31a6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Inferring on an example of the class 'coast'\n","\u001b[34m\u001b[1mclassify/predict: \u001b[0mweights=['runs/train-cls/exp4/weights/best.pt'], source=/content/drive/MyDrive/Capstone_Yolov5/dataset_scene/test/coast/natu938_jpg.rf.d222530f3475e0bd5e92ba8d3b34ac84.jpg, data=data/coco128.yaml, imgsz=[224, 224], device=, view_img=False, save_txt=False, nosave=False, augment=False, visualize=False, update=False, project=runs/predict-cls, name=exp, exist_ok=False, half=False, dnn=False, vid_stride=1\n","\u001b[31m\u001b[1mrequirements:\u001b[0m /content/requirements.txt not found, check failed.\n","YOLOv5 🚀 v7.0-151-g3e14883 Python-3.9.16 torch-2.0.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Fusing layers... \n","Model summary: 117 layers, 4176936 parameters, 0 gradients, 10.4 GFLOPs\n","image 1/1 /content/drive/MyDrive/Capstone_Yolov5/dataset_scene/test/coast/natu938_jpg.rf.d222530f3475e0bd5e92ba8d3b34ac84.jpg: 224x224 coast 0.90, mountain 0.02, highway 0.02, tallbuilding 0.02, forest 0.01, 3.7ms\n","Speed: 0.3ms pre-process, 3.7ms inference, 5.6ms NMS per image at shape (1, 3, 224, 224)\n","Results saved to \u001b[1mruns/predict-cls/exp4\u001b[0m\n"]}]},{"cell_type":"markdown","source":["We can see the inference results show ~3ms inference and the respective classes predicted probabilities."],"metadata":{"id":"DdGuG-1kNjWT"}},{"cell_type":"markdown","source":["## (OPTIONAL) Improve Our Model with Active Learning\n","\n","Now that we've trained our model once, we will want to continue to improve its performance. Improvement is largely dependent on improving our dataset.\n","\n","We can programmatically upload example failure images back to our custom dataset based on conditions (like seeing an underrpresented class or a low confidence score) using the same `pip` package."],"metadata":{"id":"I38IM6NXKNN9"}},{"cell_type":"code","source":["# # Upload example image\n","# project.upload(image_path)\n"],"metadata":{"id":"HycgSEnYKo0J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Example upload code \n","# min_conf = float(\"inf\")\n","# for pred in results:\n","#     if pred[\"score\"] < min_conf:\n","#         min_conf = pred[\"score\"]\n","# if min_conf < 0.4:\n","#     project.upload(image_path)"],"metadata":{"id":"VwXDoz_vLK3V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# (BONUS) YOLOv5 classify/predict.py Accepts Several Input Methods\n","- Webcam: `python classify/predict.py --weights yolov5s-cls.pt --source 0`\n","- Image `python classify/predict.py --weights yolov5s-cls.pt --source img.jpg`\n","- Video: `python classify/predict.py --weights yolov5s-cls.pt --source vid.mp4`\n","- Directory: `python classify/predict.py --weights yolov5s-cls.pt --source path/`\n","- Glob: `python classify/predict.py --weights yolov5s-cls.pt --source 'path/*.jpg'`\n","- YouTube: `python classify/predict.py --weights yolov5s-cls.pt --source 'https://youtu.be/Zgi9g1ksQHc'`\n","- RTSP, RTMP, HTTP stream: `python classify/predict.py --weights yolov5s-cls.pt --source 'rtsp://example.com/media.mp4'`"],"metadata":{"id":"aYlfaHDusN-j"}},{"cell_type":"markdown","source":["###Directory Example"],"metadata":{"id":"iKSP-SNTvcLJ"}},{"cell_type":"code","source":["#Directory infer\n","os.environ[\"TEST_CLASS_PATH\"] = test_class_path = os.path.join(*os.environ[\"TEST_IMAGE_PATH\"].split(os.sep)[:-1])\n","print(f\"Infering on all images from the directory {os.environ['TEST_CLASS_PATH']}\")\n","!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source /$TEST_CLASS_PATH/"],"metadata":{"id":"lwSoHcHcvjeD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###YouTube Example"],"metadata":{"id":"kCCao9t8se8i"}},{"cell_type":"code","source":["#YouTube infer\n","!python classify/predict.py --weights runs/train-cls/exp/weights/best.pt --source 'https://www.youtube.com/watch?v=7AlYA4ItA74'"],"metadata":{"id":"heebjpJBsakV"},"execution_count":null,"outputs":[]}]}